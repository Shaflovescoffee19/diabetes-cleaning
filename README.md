# ğŸ©º Data Cleaning & Feature Engineering â€” Diabetes Dataset

Raw medical data is almost never ready for modelling. This project takes a widely-used clinical dataset full of real-world messiness â€” impossible zero values masquerading as missing data, outliers that could distort every downstream analysis, and raw measurements that don't yet capture the biology they're meant to represent â€” and transforms it into a clean, feature-rich dataset ready for machine learning.

---

## ğŸ“Œ Project Snapshot

| | |
|---|---|
| **Dataset** | Pima Indians Diabetes Dataset |
| **Records** | 768 patients |
| **Features** | 8 raw â†’ 12 after engineering |
| **Target** | Diabetes diagnosis (binary) |
| **Libraries** | `pandas` Â· `numpy` Â· `scikit-learn` Â· `matplotlib` Â· `seaborn` |

---

## ğŸ—‚ï¸ The Dataset

The Pima Indians Diabetes dataset records clinical measurements from female patients of Pima Indian heritage â€” a population with elevated rates of type 2 diabetes. The dataset presents a classic real-world data quality problem: five features use zero as a placeholder for missing measurements, which is biologically impossible for values like blood pressure, BMI, and glucose. Accepting these zeros at face value would corrupt any model trained on the data.

**Features:**
`Pregnancies` Â· `Glucose` Â· `BloodPressure` Â· `SkinThickness` Â· `Insulin` Â· `BMI` Â· `DiabetesPedigreeFunction` Â· `Age`

---

## ğŸ”§ Cleaning Pipeline

### Step 1 â€” Detecting Impossible Zeros
Five columns cannot legitimately be zero in a living patient: `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`. Zero entries in these columns are missing values entered as zeros â€” a common data entry convention in older clinical datasets.

| Feature | Zero Count | % of Records |
|---------|-----------|--------------|
| Glucose | 5 | 0.7% |
| BloodPressure | 35 | 4.6% |
| SkinThickness | 227 | 29.6% |
| Insulin | 374 | 48.7% |
| BMI | 11 | 1.4% |

Insulin is missing for nearly half the dataset â€” naive deletion would lose almost half the data entirely.

### Step 2 â€” Group-Wise Median Imputation
Rather than replacing missing values with the overall column median, missing values are imputed separately within each outcome group (diabetic vs non-diabetic). This preserves the class-specific distributions â€” diabetic patients have characteristically different insulin and glucose profiles, and pooling the two groups for imputation would dilute that signal.

### Step 3 â€” Outlier Detection and Capping
IQR-based outlier detection flags values below Q1 âˆ’ 1.5Ã—IQR or above Q3 + 1.5Ã—IQR. Rather than removing entire rows (which loses other valid measurements), extreme values are capped at the IQR boundaries. This neutralises distortion while keeping every patient in the dataset.

### Step 4 â€” Feature Engineering
Four new features are derived from existing measurements to better capture the underlying biology:

| New Feature | Derivation | Rationale |
|-------------|-----------|-----------|
| `BMI_Category` | BMI binned into Underweight / Normal / Overweight / Obese | Captures the non-linear risk jump at obesity threshold |
| `Age_Group` | Age binned into Young / Middle / Senior | Captures age-related risk stages |
| `Glucose_Category` | Glucose binned into Normal / Prediabetic / Diabetic range | Directly encodes clinical glucose classification |
| `Glucose_Insulin_Ratio` | Glucose Ã· Insulin | Proxy for insulin resistance â€” a key diabetes mechanism |

### Step 5 â€” Scaling and Splitting
Min-Max scaling applied to all numerical features, followed by an 80/20 stratified train/test split. Stratification ensures the proportion of diabetic patients is preserved in both splits.

---

## ğŸ“ˆ Visualisations Generated

| File | Description |
|------|-------------|
| `plot1_missing_zeros.png` | Zero count per feature before cleaning |
| `plot2_before_after_distributions.png` | Feature distributions before and after imputation |
| `plot3_outlier_boxplots.png` | Outlier visualisation before and after capping |
| `plot4_engineered_features.png` | New feature distributions vs outcome |
| `plot5_correlation_heatmap.png` | Full correlation matrix after cleaning |
| `plot6_train_test_split.png` | Class balance in train and test splits |

---

## ğŸ” Key Findings

- Glucose is the strongest predictor of diabetes outcome (Pearson r â‰ˆ 0.49) â€” both before and after cleaning
- Obese patients show dramatically higher diabetes rates than overweight or normal BMI patients â€” the engineered `BMI_Category` captures this threshold effect that raw BMI misses
- Insulin was missing for 48.7% of records â€” group-wise median imputation recovers this signal without introducing bias toward either class
- The `Glucose_Insulin_Ratio` interaction feature captures insulin resistance patterns invisible in either feature individually

---

## ğŸ“‚ Repository Structure

```
diabetes-cleaning/
â”œâ”€â”€ diabetes.csv
â”œâ”€â”€ diabetes_cleaning.py
â”œâ”€â”€ plot1_missing_zeros.png
â”œâ”€â”€ plot2_before_after_distributions.png
â”œâ”€â”€ plot3_outlier_boxplots.png
â”œâ”€â”€ plot4_engineered_features.png
â”œâ”€â”€ plot5_correlation_heatmap.png
â”œâ”€â”€ plot6_train_test_split.png
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup

```bash
git clone https://github.com/Shaflovescoffee19/diabetes-cleaning.git
cd diabetes-cleaning
pip3 install pandas numpy scikit-learn matplotlib seaborn
python3 diabetes_cleaning.py
```

---

## ğŸ“š Skills Developed

- Identifying and handling missing data encoded as impossible values â€” a common clinical data quality problem
- Understanding the three types of missingness (MCAR, MAR, MNAR) and choosing imputation strategy accordingly
- Group-wise imputation to preserve class-specific feature distributions
- IQR-based outlier detection and capping vs removal trade-offs
- Feature engineering â€” binning, interaction terms, and domain-driven transformations
- Min-Max scaling and StandardScaler â€” when to use each and why scaling matters
- Stratified train/test splitting and the importance of preserving class ratios

---

## ğŸ—ºï¸ Learning Roadmap

**Project 2 of 10** â€” a structured series building from data exploration through to advanced ML techniques.

| # | Project | Focus |
|---|---------|-------|
| 1 | Heart Disease EDA | Exploratory analysis, visualisation |
| 2 | **Diabetes Data Cleaning** â† | Missing data, outliers, feature engineering |
| 3 | Cancer Risk Classification | Supervised learning, model comparison |
| 4 | Survival Analysis | Time-to-event modelling, Cox regression |
| 5 | Customer Segmentation | Clustering, unsupervised learning |
| 6 | Gene Expression Clustering | High-dimensional data, heatmaps |
| 7 | Explainable AI with SHAP | Model interpretability |
| 8 | Counterfactual Explanations | Actionable predictions |
| 9 | Multi-Modal Data Fusion | Stacking, ensemble methods |
| 10 | Transfer Learning | Neural networks, domain adaptation |
